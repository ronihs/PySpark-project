{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad46f51",
   "metadata": {},
   "source": [
    "### PySpark - Project \n",
    "#### Extract - Transform - Load (ETL) with Spark & Build Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36e9f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: C:\\Users\\roni2\\miniconda3\\envs\\pysparkenv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# WINDOWS PYTHON PATH \n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable          \n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable  \n",
    "\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2807a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data disimpan ke output/titanic_cleaned\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+----------+-------+-------+--------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Deck|FamilySize|IsAlone|  Title|        processed_at|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+----------+-------+-------+--------------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|null|         2|      0|    Mr.|2025-11-24 17:06:...|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|   C|         2|      0|   Mrs.|2025-11-24 17:06:...|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|null|         1|      1|  Miss.|2025-11-24 17:06:...|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|   C|         2|      0|   Mrs.|2025-11-24 17:06:...|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|null|         1|      1|    Mr.|2025-11-24 17:06:...|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|null|         1|      1|    Mr.|2025-11-24 17:06:...|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|   E|         1|      1|    Mr.|2025-11-24 17:06:...|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|null|         5|      0|Master.|2025-11-24 17:06:...|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|null|         3|      0|   Mrs.|2025-11-24 17:06:...|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|null|         2|      0|   Mrs.|2025-11-24 17:06:...|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+----------+-------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.io import read_csv, write_parquet\n",
    "from utils.transform import (\n",
    "    clean_age, fill_missing_embarked, extract_deck,\n",
    "    create_family_size, create_is_alone,\n",
    "    extract_title, simplify_title, add_timestamp\n",
    ")\n",
    "\n",
    "# 1. Baca data\n",
    "df = read_csv(\"data/titanic.csv\")\n",
    "\n",
    "# 2. Full ETL Pipeline \n",
    "df_clean = (df\n",
    "    .transform(clean_age)                  # ← pakai .transform() kfungsi return DataFrame\n",
    "    .transform(fill_missing_embarked)\n",
    "    .transform(extract_deck)\n",
    "    .transform(create_family_size)\n",
    "    .transform(create_is_alone)\n",
    "    .transform(extract_title)\n",
    "    .transform(simplify_title)\n",
    "    .transform(add_timestamp)\n",
    ")\n",
    "\n",
    "# 3. Simpan hasil\n",
    "write_parquet(df_clean, \"output/titanic_cleaned\", partitionBy=\"Pclass\")\n",
    "\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c664a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siap training model!\n"
     ]
    }
   ],
   "source": [
    "# ================== 1. Import Library ML ==================\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "print(\"Siap training model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddcca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data siap training: 185 baris (setelah drop NA)\n"
     ]
    }
   ],
   "source": [
    "# ================== 2. Siapkan Data untuk ML ==================\n",
    "# Pilih kolom yang akan jadi fitur\n",
    "df_ml = df_clean.select(\n",
    "    \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\",\n",
    "    \"Embarked\", \"FamilySize\", \"IsAlone\", \"Title\", \"Deck\"\n",
    ").na.drop()  # drop baris yang ada null (Aman karena Age & Deck sudah di-handle)\n",
    "\n",
    "print(f\"Data siap training: {df_ml.count()} baris (setelah drop NA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a586b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 3. Pipeline Preprocessing + Model ==================\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\")\n",
    "    for col in [\"Sex\", \"Embarked\", \"Title\", \"Deck\"]\n",
    "]\n",
    "\n",
    "# OneHotEncoder\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_vec\")\n",
    "    for col in [\"Sex\", \"Embarked\", \"Title\", \"Deck\"]\n",
    "]\n",
    "\n",
    "# Gabung semua fitur jadi 1 kolom \"features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"IsAlone\",\n",
    "        \"Sex_vec\", \"Embarked_vec\", \"Title_vec\", \"Deck_vec\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# buat Model ML, silahkan gunakan salah satu, kali ini saya gunakan RF\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", numTrees=200, maxDepth=10, seed=42)\n",
    "# lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")  # kalau mau coba yang lebih simpel\n",
    "\n",
    "# 5. Full Pipeline ETL +ML\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9981831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data siap ML: 714 baris\n",
      "+----+-----+\n",
      "|Deck|count|\n",
      "+----+-----+\n",
      "|   G|    4|\n",
      "|   F|   11|\n",
      "|   T|    1|\n",
      "|   C|   51|\n",
      "|   B|   45|\n",
      "|   D|   31|\n",
      "|   A|   12|\n",
      "|null|  529|\n",
      "|   E|   30|\n",
      "+----+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|    Title|count|\n",
      "+---------+-----+\n",
      "|      Mr.|  398|\n",
      "|    Miss.|  146|\n",
      "|  Master.|   36|\n",
      "|    Capt.|    1|\n",
      "|Jonkheer.|    1|\n",
      "|      Ms.|    1|\n",
      "|   Major.|    2|\n",
      "|    Mlle.|    2|\n",
      "|  Unknown|    1|\n",
      "|     Mrs.|  108|\n",
      "|     Don.|    1|\n",
      "|     Rev.|    6|\n",
      "|      Dr.|    6|\n",
      "|    Lady.|    1|\n",
      "|     Mme.|    1|\n",
      "|     Sir.|    1|\n",
      "|     Col.|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================Bersihkan Deck & Title yang kosong ==================\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_ml = df_clean.select(\n",
    "    \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\",\n",
    "    \"Embarked\", \"FamilySize\", \"IsAlone\", \"Title\", \"Deck\"\n",
    ").na.drop(subset=[\"Age\", \"Embarked\"]) \\\n",
    " .withColumn(\"Deck\", when(col(\"Deck\") == \"\", \"Unknown\").otherwise(col(\"Deck\"))) \\\n",
    " .withColumn(\"Title\", when(col(\"Title\") == \"\", \"Unknown\").otherwise(col(\"Title\")))\n",
    "\n",
    "print(f\"Data siap ML: {df_ml.count()} baris\")\n",
    "df_ml.groupBy(\"Deck\").count().show()\n",
    "df_ml.groupBy(\"Title\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ce3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== StringIndexer ==================\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"Sex\",       outputCol=\"Sex_idx\",       handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_idx\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Title\",     outputCol=\"Title_idx\",     handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Deck\",      outputCol=\"Deck_idx\",      handleInvalid=\"keep\")\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=\"Sex_idx\",       outputCol=\"Sex_vec\"),\n",
    "    OneHotEncoder(inputCol=\"Embarked_idx\", outputCol=\"Embarked_vec\"),\n",
    "    OneHotEncoder(inputCol=\"Title_idx\",     outputCol=\"Title_vec\"),\n",
    "    OneHotEncoder(inputCol=\"Deck_idx\",      outputCol=\"Deck_vec\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb5b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SELESAI!\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"FamilySize\",\"IsAlone\",\n",
    "               \"Sex_vec\",\"Embarked_vec\",\"Title_vec\",\"Deck_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\",\n",
    "                            numTrees=200, maxDepth=10, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "\n",
    "# Split & Train\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train)   # SEKARANG PASTI JALAN!\n",
    "print(\"Training SELESAI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e61c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config model disimpan! \n"
     ]
    }
   ],
   "source": [
    "# 1. Simpan parameter\n",
    "import json\n",
    "import os\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "model_info = {\n",
    "    \"feature_cols\": [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"FamilySize\",\"IsAlone\",\"Title\",\"Deck\"],\n",
    "    \"rf_params\": {\n",
    "        \"numTrees\": 200,\n",
    "        \"maxDepth\": 10,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"model/titanic_config.json\", \"w\") as f:\n",
    "    json.dump(model_info, f)\n",
    "\n",
    "print(\"Config model disimpan! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1272357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil direbuild!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "with open(\"model/titanic_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Buat pipeline\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") \n",
    "            for c in [\"Sex\",\"Embarked\",\"Title\",\"Deck\"]]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\") \n",
    "            for c in [\"Sex\",\"Embarked\",\"Title\",\"Deck\"]]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"FamilySize\",\"IsAlone\",\n",
    "               \"Sex_vec\",\"Embarked_vec\",\"Title_vec\",\"Deck_vec\"],\n",
    "    outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", **config[\"rf_params\"])\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "model = pipeline.fit(df_ml)   # cuma 2–3 detik!\n",
    "\n",
    "print(\"Model berhasil direbuild!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f647e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config model berhasil disimpan!\n"
     ]
    }
   ],
   "source": [
    "# Simpan config\n",
    "import json, os\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"note\": \"Titanic PySpark Model - Rebuild cuma 2 detik!\",\n",
    "    \"rf_params\": {\"numTrees\": 200, \"maxDepth\": 10, \"seed\": 42}\n",
    "}\n",
    "\n",
    "with open(\"model/deploy_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Config model berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37699b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediksi siap!\n"
     ]
    }
   ],
   "source": [
    "#load Model\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 1. Load config\n",
    "with open(\"model/deploy_config.json\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# 2. Buat pipeline \n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") \n",
    "            for c in [\"Sex\",\"Embarked\",\"Title\",\"Deck\"]]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_vec\") \n",
    "            for c in [\"Sex\",\"Embarked\",\"Title\",\"Deck\"]]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"FamilySize\",\"IsAlone\",\n",
    "               \"Sex_vec\",\"Embarked_vec\",\"Title_vec\",\"Deck_vec\"],\n",
    "    outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\", **cfg[\"rf_params\"])\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
    "model = pipeline.fit(df_ml)\n",
    "\n",
    "print(\"Model prediksi siap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6298f76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------+\n",
      "|prediction|probability                              |\n",
      "+----------+-----------------------------------------+\n",
      "|0.0       |[0.7390814181250815,0.2609185818749184]  |\n",
      "|1.0       |[0.022943965423918754,0.9770560345760811]|\n",
      "+----------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.spark_session import spark  \n",
    "\n",
    "# buat data testing model (JACK & ROSE)\n",
    "jack_rose = spark.createDataFrame([\n",
    "    (3, \"male\",   23.0, 0, 0, 7.25,     \"S\", 1, 1, \"Mr\",    \"Unknown\"),  # Jack Dawson\n",
    "    (1, \"female\", 19.0, 1, 0, 512.3292, \"C\", 2, 0, \"Miss\", \"B\")         # Rose DeWitt Bukater\n",
    "], [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"FamilySize\",\"IsAlone\",\"Title\",\"Deck\"])\n",
    "\n",
    "# PREDIKSI!\n",
    "hasil = model.transform(jack_rose)\n",
    "\n",
    "# TAMPILKAN HASILNYA CANTIK\n",
    "hasil.select(\"prediction\", \"probability\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysparkenv]",
   "language": "python",
   "name": "conda-env-pysparkenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
